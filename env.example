# Copy this file to .env.local and fill in your actual API keys

# OpenAI API Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Pinecone Configuration
# Get your API key from: https://app.pinecone.io/
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=knowledgebase-index

# Application Settings
EMBEDDING_MODEL=text-embedding-3-small
# Dimensions must match your Pinecone index dimensions
# text-embedding-3-small supports: 512, 1024, 1536 (default)
# text-embedding-3-large supports: 256, 1024, 3072 (default)
# text-embedding-ada-002: always 1536 (no custom dimensions)
EMBEDDING_DIMENSIONS=1024

# Chunking Configuration
CHUNK_SIZE=600           # Target tokens per chunk
CHUNK_OVERLAP=0.2        # 20% overlap between chunks (0.0 - 0.5)

# Retrieval Configuration
SIMILARITY_THRESHOLD=0.7      # Minimum similarity score (0.0 - 1.0)
FALLBACK_THRESHOLD=0.5        # Fallback threshold if no results meet primary threshold
USE_MMR=true                  # Enable MMR (Maximal Marginal Relevance) for diverse results
MMR_FETCH_K=20               # Number of candidates to fetch for MMR
MMR_LAMBDA=0.5               # MMR diversity parameter (0=max diversity, 1=max relevance)
MAX_CONTEXT_TOKENS=2000      # Maximum tokens to send to LLM as context

# Cache Configuration
CACHE_ENABLED=true           # Enable query result caching
QUERY_CACHE_TTL=300000       # Cache TTL in milliseconds (300000 = 5 minutes)
QUERY_CACHE_SIZE=50          # Maximum number of cached queries
